{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "v3.1_LastActiv(Add).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/walexand3r/4YP/blob/master/CIFAR10/ML/hypoML%20Testing/v1.0_LastActiv(Add).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgeRXODCpXnN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Forming 2 networks, one for D and one for +, combining by adding before final activation layer\n",
        "\n",
        "Model Version 1.0\n",
        "\n",
        "Added: \n",
        "\n",
        "- Made all processes into functions, making the four tests easier to run\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcOpH2s7jxyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# This block imports relevant packages and sets intial seed to ensure reproducability\n",
        "# Seed value\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os, datetime\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(seed_value)\n",
        "# for later versions: tf.random.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.keras import backend as K\n",
        "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "# K.set_session(sess)\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten,  Reshape, Activation\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LeakyReLU, Softmax, Concatenate\n",
        "from tensorflow.keras.layers import Maximum, Add\n",
        "from tensorflow import math\n",
        "\n",
        "%load_ext tensorboard\n",
        "import pandas as pd\n",
        "\n",
        "# Mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zuFOTq9KixG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sets paths for train/test D/D+\n",
        "folder = 'RGBHSV/32x64'\n",
        "path = '/content/drive/My Drive/4th Year/4YP/CIFAR10/Processed Datasets/'\n",
        "\n",
        "# Sets results path\n",
        "saveFolder = 'LastActivation/Add/'\n",
        "savePath = '/content/drive/My Drive/4th Year/4YP/CIFAR10/Results/hypoML Testing/' + saveFolder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MuaPOiAXxnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createModel():\n",
        "  batch_size = 64\n",
        "  epochs = 20\n",
        "  num_classes = 10\n",
        "\n",
        "  # Forming 2 input models, 'left' and 'right'\n",
        "  # left\n",
        "  leftInputs = keras.Input(shape=(32, 32, 3))\n",
        "  leftx = Conv2D(64, kernel_size = (3,3), padding = 'same')(leftInputs)\n",
        "  leftx = LeakyReLU(alpha = 0.3)(leftx)\n",
        "\n",
        "  leftx = Conv2D(64, kernel_size=(3, 3), padding = 'same')(leftx)\n",
        "  leftx = LeakyReLU(alpha = 0.3)(leftx)\n",
        "  leftx = MaxPooling2D(pool_size=(2,2))(leftx)\n",
        "\n",
        "  leftx = Conv2D(128, kernel_size=(3, 3), padding = 'same')(leftx)\n",
        "  leftx = LeakyReLU(alpha = 0.3)(leftx)\n",
        "\n",
        "  leftx = Conv2D(128, kernel_size=(3, 3), padding = 'same')(leftx)\n",
        "  leftx = LeakyReLU(alpha = 0.3)(leftx)\n",
        "  leftx = MaxPooling2D(pool_size=(2,2))(leftx)\n",
        "\n",
        "  leftx = Conv2D(256, kernel_size=(3, 3), padding = 'same')(leftx)\n",
        "  leftx = LeakyReLU(alpha = 0.3)(leftx)\n",
        "\n",
        "  leftx = Conv2D(256, kernel_size=(1, 1), padding = 'same')(leftx)\n",
        "  leftx = LeakyReLU(alpha = 0.3)(leftx)\n",
        "  leftx = Dropout(rate = 0.5)(leftx)\n",
        "  leftx = Conv2D(10, kernel_size=(3, 3), padding = 'same')(leftx)\n",
        "  leftx = LeakyReLU(alpha = 0.3)(leftx)\n",
        "  leftx = tf.reduce_mean(leftx,(1,2))\n",
        "\n",
        "  # right\n",
        "  rightInputs = keras.Input(shape=(32, 32, 3))\n",
        "  rightx = Conv2D(64, kernel_size = (3,3), padding = 'same')(rightInputs)\n",
        "  rightx = LeakyReLU(alpha = 0.3)(rightx)\n",
        "\n",
        "  rightx = Conv2D(64, kernel_size=(3, 3), padding = 'same')(rightx)\n",
        "  rightx = LeakyReLU(alpha = 0.3)(rightx)\n",
        "  rightx = MaxPooling2D(pool_size=(2,2))(rightx)\n",
        "\n",
        "  rightx = Conv2D(128, kernel_size=(3, 3), padding = 'same')(rightx)\n",
        "  rightx = LeakyReLU(alpha = 0.3)(rightx)\n",
        "\n",
        "  rightx = Conv2D(128, kernel_size=(3, 3), padding = 'same')(rightx)\n",
        "  rightx = LeakyReLU(alpha = 0.3)(rightx)  \n",
        "  rightx = MaxPooling2D(pool_size=(2,2))(rightx)\n",
        "\n",
        "  rightx = Conv2D(256, kernel_size=(3, 3), padding = 'same')(rightx)\n",
        "  rightx = LeakyReLU(alpha = 0.3)(rightx)\n",
        "\n",
        "  rightx = Conv2D(256, kernel_size=(1, 1), padding = 'same')(rightx)\n",
        "  rightx = LeakyReLU(alpha = 0.3)(rightx)\n",
        "  rightx = Dropout(rate = 0.5)(rightx)\n",
        "  rightx = Conv2D(10, kernel_size=(3, 3), padding = 'same')(rightx)\n",
        "  rightx = LeakyReLU(alpha = 0.3)(rightx)\n",
        "  rightx = tf.reduce_mean(rightx,(1,2))\n",
        "\n",
        "  # merging\n",
        "  x = Add()([leftx,rightx])\n",
        "  outputs = Softmax()(x)\n",
        "\n",
        "  model = keras.Model(inputs=(leftInputs, rightInputs), outputs=outputs, name='cifar10_model')\n",
        "\n",
        "  model.compile(loss = keras.losses.categorical_crossentropy,\n",
        "                optimizer = keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.98),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "  return model, logdir, tensorboard_callback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO9peEe488HY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createRawData(xTestL,yTestL,xTestR,model):\n",
        "  # This function makes predictions on test data, compares to real class labels\n",
        "  # and constructs output data to display this\n",
        "\n",
        "  n_test = len(yTestL)\n",
        "  yTestL = np.argmax(yTestL,axis=1)\n",
        "  print('Making Predictions')\n",
        "  # Make predictions\n",
        "  probs = model.predict([xTestL,xTestR], \n",
        "                        batch_size=batch_size,\n",
        "                        verbose=1,)\n",
        "  predictions = np.argmax(probs,axis = 1)\n",
        "  # Forming into required raw data format \n",
        "  Correctness = np.zeros(n_test)\n",
        "  for i in range(len(predictions)):\n",
        "    if np.array_equal(predictions[i],yTestL[i]):\n",
        "      Correctness[i] = True\n",
        "    else:\n",
        "      Correctness[i] = False\n",
        "\n",
        "  GroundTruth= yTestL\n",
        "  RawData = np.zeros((n_test,5))\n",
        "  RawData[:,1] = GroundTruth\n",
        "  RawData[:,2] = predictions\n",
        "  RawData[:,4] = Correctness\n",
        "\n",
        "  for i in range(n_test):\n",
        "    RawData[i,0] = i+1\n",
        "  #print(sum(Correctness))\n",
        "  return RawData"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAMtSRA_Fac2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def importData(trainingOn,testingOn):\n",
        "  # Data Import\n",
        "  # All values are scaled to range 0-1\n",
        "  print('Loading Datasets')\n",
        "\n",
        "  ### Left data \n",
        "  # Whatever test is being run, left data is always cifar10 RGB\n",
        "  (xTrainL, yTrainL), (xTestL, yTestL) = cifar10.load_data()\n",
        "  n_train = np.size(xTrainL,0)\n",
        "  n_test = np.size(xTestL,0)\n",
        "  num_classes = 10\n",
        "\n",
        "  xTrainL = xTrainL / 255\n",
        "  xTestL = xTestL / 255\n",
        " \n",
        "  # One-hot encoding class labels\n",
        "  yTrainL = keras.utils.to_categorical(yTrainL, num_classes)\n",
        "  yTestL = keras.utils.to_categorical(yTestL, num_classes)\n",
        "\n",
        "  # input image dimensions\n",
        "  row_length = np.size(xTrainL,1) * np.size(xTrainL,2) * np.size(xTrainL,3) + 1\n",
        "  img_rows, img_cols = np.size(xTrainL,1), np.size(xTrainL,2)\n",
        "\n",
        "  ### Right data\n",
        "  # Right data depends on what test is being run\n",
        "  print('Importing csv data')\n",
        "  if trainingOn == 'M':\n",
        "    loadPath = path + folder + '/+/+_noise_train.csv'\n",
        "    TrainR = pd.read_csv(loadPath,header=None,engine='python')\n",
        "  elif trainingOn == 'M+':\n",
        "    loadPath = path + folder + '/+/+_train.csv'\n",
        "    TrainR = pd.read_csv(loadPath,header=None,engine='python')\n",
        "  else:\n",
        "    print('Invalid input')\n",
        "\n",
        "  if testingOn == 'D':\n",
        "    loadPath = path + folder + '/+/+_noise_test.csv'\n",
        "    TestR = pd.read_csv(loadPath,header=None,engine='python')\n",
        "  elif testingOn == 'D+':\n",
        "    loadPath = path + folder + '/+/+_test.csv'\n",
        "    TestR = pd.read_csv(loadPath,header=None,engine='python')\n",
        "  else:\n",
        "    print('Invalid input')\n",
        "  print('Done')\n",
        "\n",
        "  # Extracting data\n",
        "  yTestR = TestR.iloc[:,0]\n",
        "  xTestR = TestR.iloc[:,1:]\n",
        "  yTrainR = TrainR.iloc[:,0]\n",
        "  xTrainR = TrainR.iloc[:,1:]\n",
        "  xTrainR = xTrainR.values.reshape(n_train,img_rows,img_cols,3)\n",
        "  xTestR = xTestR.values.reshape(n_test,img_rows,img_cols,3)\n",
        "\n",
        "  # convert class vectors to binary class matrices\n",
        "  yTrainR = keras.utils.to_categorical(yTrainR, num_classes)\n",
        "  yTestR = keras.utils.to_categorical(yTestR, num_classes)\n",
        "\n",
        "  print('Data created')\n",
        "\n",
        "  return xTrainL,yTrainL,xTrainR,yTrainR,xTestL,yTestL,xTestR,yTestR\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYod8Lp3KZpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test 1: R(M,D)     \n",
        "\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "num_classes = 10\n",
        "(xTrainL,yTrainL,xTrainR,yTrainR,xTestL,yTestL,xTestR,yTestR) = importData('M','D')\n",
        "\n",
        "model, logdir, tensorboard_callback = createModel()\n",
        "\n",
        "model.fit((xTrainL,xTrainR),\n",
        "          yTrainL,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_split = 0.02,\n",
        "          callbacks = [tensorboard_callback])\n",
        "\n",
        "RawData_1 = createRawData(xTestL,yTestL,xTestR,model)\n",
        "\n",
        "print('Saving')\n",
        "# Saving\n",
        "savePath1 = savePath + 'R(M,D).csv'\n",
        "pd.DataFrame(RawData_1).to_csv(savePath1,header=None, index=None)\n",
        "\n",
        "# test_scores = model.evaluate([xTestL,xTestR], yTestL, verbose=2)\n",
        "# print('Test loss:', test_scores[0])\n",
        "# print('Test accuracy:', test_scores[1])\n",
        "print(sum(RawData_1[:,4]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Us18DLWLeUhi",
        "colab": {}
      },
      "source": [
        "# Test 2: R(M,D+)     \n",
        "\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "num_classes = 10\n",
        "(xTrainL,yTrainL,xTrainR,yTrainR,xTestL,yTestL,xTestR,yTestR) = importData('M','D+')\n",
        "\n",
        "model, logdir, tensorboard_callback = createModel()\n",
        "\n",
        "model.fit((xTrainL,xTrainR),\n",
        "          yTrainL,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_split = 0.02,\n",
        "          callbacks = [tensorboard_callback])\n",
        "\n",
        "RawData_2 = createRawData(xTestL,yTestL,xTestR,model)\n",
        "\n",
        "print('Saving')\n",
        "# Saving\n",
        "savePath2 = savePath + 'R(M,D+).csv'\n",
        "pd.DataFrame(RawData_2).to_csv(savePath2,header=None, index=None)\n",
        "\n",
        "# test_scores = model.evaluate([xTestL,xTestR], yTestL, verbose=2)\n",
        "# print('Test loss:', test_scores[0])\n",
        "# print('Test accuracy:', test_scores[1])\n",
        "print(sum(RawData_2[:,4]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xH3v-k9KeU0b",
        "colab": {}
      },
      "source": [
        "# Test 3: R(M+,D)     \n",
        "\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "num_classes = 10\n",
        "(xTrainL,yTrainL,xTrainR,yTrainR,xTestL,yTestL,xTestR,yTestR) = importData('M+','D')\n",
        "\n",
        "model, logdir, tensorboard_callback = createModel()\n",
        "\n",
        "model.fit((xTrainL,xTrainR),\n",
        "          yTrainL,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_split = 0.02,\n",
        "          callbacks = [tensorboard_callback])\n",
        "\n",
        "RawData_3 = createRawData(xTestL,yTestL,xTestR,model)\n",
        "\n",
        "print('Saving')\n",
        "# Saving\n",
        "savePath3 = savePath + 'R(M+,D).csv'\n",
        "pd.DataFrame(RawData_3).to_csv(savePath3,header=None, index=None)\n",
        "\n",
        "# test_scores = model.evaluate([xTestL,xTestR], yTestL, verbose=2)\n",
        "# print('Test loss:', test_scores[0])\n",
        "# print('Test accuracy:', test_scores[1])\n",
        "print(sum(RawData_3[:,4]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hX8SKI5xeVB4",
        "colab": {}
      },
      "source": [
        "# Test 4: R(M+,D+)     \n",
        "\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "num_classes = 10\n",
        "(xTrainL,yTrainL,xTrainR,yTrainR,xTestL,yTestL,xTestR,yTestR) = importData('M+','D+')\n",
        "\n",
        "model, logdir, tensorboard_callback = createModel()\n",
        "\n",
        "model.fit((xTrainL,xTrainR),\n",
        "          yTrainL,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_split = 0.02,\n",
        "          callbacks = [tensorboard_callback])\n",
        "\n",
        "RawData_4 = createRawData(xTestL,yTestL,xTestR,model)\n",
        "\n",
        "print('Saving')\n",
        "# Saving\n",
        "savePath4 = savePath + 'R(M+,D+).csv'\n",
        "pd.DataFrame(RawData_4).to_csv(savePath4,header=None, index=None)\n",
        "\n",
        "# test_scores = model.evaluate([xTestL,xTestR], yTestL, verbose=2)\n",
        "# print('Test loss:', test_scores[0])\n",
        "# print('Test accuracy:', test_scores[1])\n",
        "print(sum(RawData_4[:,4]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FV_5MAlupMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill 725\n",
        "%tensorboard --logdir logs  # start Tensorboard\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
